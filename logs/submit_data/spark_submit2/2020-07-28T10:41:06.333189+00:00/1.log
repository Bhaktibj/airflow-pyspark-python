[2020-07-28 16:36:47,626] {taskinstance.py:669} INFO - Dependencies all met for <TaskInstance: submit_data.spark_submit2 2020-07-28T10:41:06.333189+00:00 [queued]>
[2020-07-28 16:36:47,638] {taskinstance.py:669} INFO - Dependencies all met for <TaskInstance: submit_data.spark_submit2 2020-07-28T10:41:06.333189+00:00 [queued]>
[2020-07-28 16:36:47,638] {taskinstance.py:879} INFO - 
--------------------------------------------------------------------------------
[2020-07-28 16:36:47,638] {taskinstance.py:880} INFO - Starting attempt 1 of 1
[2020-07-28 16:36:47,638] {taskinstance.py:881} INFO - 
--------------------------------------------------------------------------------
[2020-07-28 16:36:48,385] {taskinstance.py:900} INFO - Executing <Task(SparkSubmitOperator): spark_submit2> on 2020-07-28T10:41:06.333189+00:00
[2020-07-28 16:36:48,390] {standard_task_runner.py:53} INFO - Started process 115665 to run task
[2020-07-28 16:36:48,833] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: submit_data.spark_submit2 2020-07-28T10:41:06.333189+00:00 [running]> bhakti-Vostro-3558
[2020-07-28 16:36:48,853] {logging_mixin.py:112} INFO - [2020-07-28 16:36:48,852] {base_hook.py:87} INFO - Using connection to: id: spark_local. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: None
[2020-07-28 16:36:48,854] {logging_mixin.py:112} INFO - [2020-07-28 16:36:48,854] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: spark-submit --master local --jars /home/bhakti/airflow/jars/mysql-connector-java-5.1.47-bin.jar --name airflow-spark /home/bhakti/airflow/spark_jobs/write_mysql.py
[2020-07-28 16:36:50,423] {logging_mixin.py:112} INFO - [2020-07-28 16:36:50,423] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:50 WARN Utils: Your hostname, bhakti-Vostro-3558 resolves to a loopback address: 127.0.1.1; using 192.168.43.94 instead (on interface wlp6s0)
[2020-07-28 16:36:50,424] {logging_mixin.py:112} INFO - [2020-07-28 16:36:50,424] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2020-07-28 16:36:51,181] {logging_mixin.py:112} INFO - [2020-07-28 16:36:51,181] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-07-28 16:36:53,279] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,279] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:36:53,280] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,280] {spark_submit_hook.py:479} INFO - Spark job started
[2020-07-28 16:36:53,280] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,280] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:36:53,371] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,371] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2020-07-28 16:36:53,385] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,385] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SparkContext: Running Spark version 3.0.0
[2020-07-28 16:36:53,459] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,459] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO ResourceUtils: ==============================================================
[2020-07-28 16:36:53,461] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,461] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO ResourceUtils: Resources for spark.driver:
[2020-07-28 16:36:53,461] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,461] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:36:53,462] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,461] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO ResourceUtils: ==============================================================
[2020-07-28 16:36:53,462] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,462] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SparkContext: Submitted application: connection
[2020-07-28 16:36:53,565] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,565] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SecurityManager: Changing view acls to: bhakti
[2020-07-28 16:36:53,565] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,565] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SecurityManager: Changing modify acls to: bhakti
[2020-07-28 16:36:53,566] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,566] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SecurityManager: Changing view acls groups to:
[2020-07-28 16:36:53,566] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,566] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SecurityManager: Changing modify acls groups to:
[2020-07-28 16:36:53,567] {logging_mixin.py:112} INFO - [2020-07-28 16:36:53,566] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bhakti); groups with view permissions: Set(); users  with modify permissions: Set(bhakti); groups with modify permissions: Set()
[2020-07-28 16:36:54,021] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,021] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO Utils: Successfully started service 'sparkDriver' on port 35269.
[2020-07-28 16:36:54,071] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,071] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO SparkEnv: Registering MapOutputTracker
[2020-07-28 16:36:54,120] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,119] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO SparkEnv: Registering BlockManagerMaster
[2020-07-28 16:36:54,159] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,158] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2020-07-28 16:36:54,159] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,159] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2020-07-28 16:36:54,165] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,165] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2020-07-28 16:36:54,186] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,186] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a895746e-341d-4241-9472-bed26f3e2432
[2020-07-28 16:36:54,227] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,227] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2020-07-28 16:36:54,257] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,257] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2020-07-28 16:36:54,824] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,824] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2020-07-28 16:36:54,826] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,825] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2020-07-28 16:36:54,842] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,841] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2020-07-28 16:36:54,965] {logging_mixin.py:112} INFO - [2020-07-28 16:36:54,964] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.94:4042
[2020-07-28 16:36:55,055] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,055] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO SparkContext: Added JAR file:///home/bhakti/airflow/jars/mysql-connector-java-5.1.47-bin.jar at spark://192.168.43.94:35269/jars/mysql-connector-java-5.1.47-bin.jar with timestamp 1595934415054
[2020-07-28 16:36:55,339] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,338] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO Executor: Starting executor ID driver on host 192.168.43.94
[2020-07-28 16:36:55,391] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,391] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37731.
[2020-07-28 16:36:55,393] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,393] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO NettyBlockTransferService: Server created on 192.168.43.94:37731
[2020-07-28 16:36:55,396] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,396] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2020-07-28 16:36:55,417] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,416] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.94, 37731, None)
[2020-07-28 16:36:55,427] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,427] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.94:37731 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.43.94, 37731, None)
[2020-07-28 16:36:55,434] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,433] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.94, 37731, None)
[2020-07-28 16:36:55,436] {logging_mixin.py:112} INFO - [2020-07-28 16:36:55,436] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.94, 37731, None)
[2020-07-28 16:36:56,147] {logging_mixin.py:112} INFO - [2020-07-28 16:36:56,147] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bhakti/airflow/spark-warehouse').
[2020-07-28 16:36:56,148] {logging_mixin.py:112} INFO - [2020-07-28 16:36:56,148] {spark_submit_hook.py:479} INFO - 20/07/28 16:36:56 INFO SharedState: Warehouse path is 'file:/home/bhakti/airflow/spark-warehouse'.
[2020-07-28 16:37:05,010] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,009] {spark_submit_hook.py:479} INFO - +--------------------------+------+
[2020-07-28 16:37:05,010] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,010] {spark_submit_hook.py:479} INFO - |DateTime                  |Value |
[2020-07-28 16:37:05,010] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,010] {spark_submit_hook.py:479} INFO - +--------------------------+------+
[2020-07-28 16:37:05,011] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,011] {spark_submit_hook.py:479} INFO - |2020-07-28 16:36:57.355337|Shakti|
[2020-07-28 16:37:05,011] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,011] {spark_submit_hook.py:479} INFO - |2020-07-28 16:36:57.355392|bahkti|
[2020-07-28 16:37:05,011] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,011] {spark_submit_hook.py:479} INFO - +--------------------------+------+
[2020-07-28 16:37:05,012] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,011] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:37:05,713] {logging_mixin.py:112} INFO - [2020-07-28 16:37:05,713] {spark_submit_hook.py:479} INFO - Tue Jul 28 16:37:05 IST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[2020-07-28 16:37:06,938] {logging_mixin.py:112} INFO - [2020-07-28 16:37:06,938] {spark_submit_hook.py:479} INFO - Tue Jul 28 16:37:06 IST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[2020-07-28 16:37:07,523] {logging_mixin.py:112} INFO - [2020-07-28 16:37:07,522] {spark_submit_hook.py:479} INFO - Tue Jul 28 16:37:07 IST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[2020-07-28 16:37:07,807] {logging_mixin.py:112} INFO - [2020-07-28 16:37:07,807] {spark_submit_hook.py:479} INFO - spark job is ended
[2020-07-28 16:37:08,027] {taskinstance.py:1052} INFO - Marking task as SUCCESS.dag_id=submit_data, task_id=spark_submit2, execution_date=20200728T104106, start_date=20200728T110647, end_date=20200728T110708
[2020-07-28 16:37:15,113] {logging_mixin.py:112} INFO - [2020-07-28 16:37:15,113] {local_task_job.py:103} INFO - Task exited with return code 0
