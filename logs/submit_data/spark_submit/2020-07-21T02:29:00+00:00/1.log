[2020-07-28 16:13:29,776] {taskinstance.py:669} INFO - Dependencies all met for <TaskInstance: submit_data.spark_submit 2020-07-21T02:29:00+00:00 [queued]>
[2020-07-28 16:13:30,989] {taskinstance.py:669} INFO - Dependencies all met for <TaskInstance: submit_data.spark_submit 2020-07-21T02:29:00+00:00 [queued]>
[2020-07-28 16:13:30,989] {taskinstance.py:879} INFO - 
--------------------------------------------------------------------------------
[2020-07-28 16:13:30,989] {taskinstance.py:880} INFO - Starting attempt 1 of 1
[2020-07-28 16:13:30,989] {taskinstance.py:881} INFO - 
--------------------------------------------------------------------------------
[2020-07-28 16:13:31,676] {taskinstance.py:900} INFO - Executing <Task(SparkSubmitOperator): spark_submit> on 2020-07-21T02:29:00+00:00
[2020-07-28 16:13:31,681] {standard_task_runner.py:53} INFO - Started process 111150 to run task
[2020-07-28 16:13:32,998] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: submit_data.spark_submit 2020-07-21T02:29:00+00:00 [running]> bhakti-Vostro-3558
[2020-07-28 16:13:33,242] {logging_mixin.py:112} INFO - [2020-07-28 16:13:33,242] {base_hook.py:87} INFO - Using connection to: id: spark_local. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: None
[2020-07-28 16:13:33,243] {logging_mixin.py:112} INFO - [2020-07-28 16:13:33,243] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: spark-submit --master local --name airflow-spark /home/bhakti/airflow/spark_jobs/spark_test.py
[2020-07-28 16:13:36,508] {logging_mixin.py:112} INFO - [2020-07-28 16:13:36,507] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:36 WARN Utils: Your hostname, bhakti-Vostro-3558 resolves to a loopback address: 127.0.1.1; using 192.168.43.94 instead (on interface wlp6s0)
[2020-07-28 16:13:36,509] {logging_mixin.py:112} INFO - [2020-07-28 16:13:36,509] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2020-07-28 16:13:37,386] {logging_mixin.py:112} INFO - [2020-07-28 16:13:37,386] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-07-28 16:13:55,420] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,420] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2020-07-28 16:13:55,449] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,449] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SparkContext: Running Spark version 3.0.0
[2020-07-28 16:13:55,550] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,550] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO ResourceUtils: ==============================================================
[2020-07-28 16:13:55,553] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,553] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO ResourceUtils: Resources for spark.driver:
[2020-07-28 16:13:55,554] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,554] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:13:55,555] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,555] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO ResourceUtils: ==============================================================
[2020-07-28 16:13:55,556] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,555] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SparkContext: Submitted application: Spark_Test_Python
[2020-07-28 16:13:55,681] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,681] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SecurityManager: Changing view acls to: bhakti
[2020-07-28 16:13:55,681] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,681] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SecurityManager: Changing modify acls to: bhakti
[2020-07-28 16:13:55,682] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,682] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SecurityManager: Changing view acls groups to:
[2020-07-28 16:13:55,682] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,682] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SecurityManager: Changing modify acls groups to:
[2020-07-28 16:13:55,683] {logging_mixin.py:112} INFO - [2020-07-28 16:13:55,682] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bhakti); groups with view permissions: Set(); users  with modify permissions: Set(bhakti); groups with modify permissions: Set()
[2020-07-28 16:13:57,905] {logging_mixin.py:112} INFO - [2020-07-28 16:13:57,905] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:57 INFO Utils: Successfully started service 'sparkDriver' on port 34533.
[2020-07-28 16:13:59,361] {logging_mixin.py:112} INFO - [2020-07-28 16:13:59,361] {spark_submit_hook.py:479} INFO - 20/07/28 16:13:59 INFO SparkEnv: Registering MapOutputTracker
[2020-07-28 16:14:01,786] {logging_mixin.py:112} INFO - [2020-07-28 16:14:01,785] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:01 INFO SparkEnv: Registering BlockManagerMaster
[2020-07-28 16:14:02,710] {logging_mixin.py:112} INFO - [2020-07-28 16:14:02,710] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2020-07-28 16:14:02,712] {logging_mixin.py:112} INFO - [2020-07-28 16:14:02,712] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2020-07-28 16:14:02,991] {logging_mixin.py:112} INFO - [2020-07-28 16:14:02,991] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2020-07-28 16:14:03,741] {logging_mixin.py:112} INFO - [2020-07-28 16:14:03,740] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-00d56d53-01bd-449c-b8c0-444319368220
[2020-07-28 16:14:04,190] {logging_mixin.py:112} INFO - [2020-07-28 16:14:04,190] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2020-07-28 16:14:04,504] {logging_mixin.py:112} INFO - [2020-07-28 16:14:04,504] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2020-07-28 16:14:23,367] {logging_mixin.py:112} INFO - [2020-07-28 16:14:23,367] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2020-07-28 16:14:23,367] {logging_mixin.py:112} INFO - [2020-07-28 16:14:23,367] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2020-07-28 16:14:23,382] {logging_mixin.py:112} INFO - [2020-07-28 16:14:23,381] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:23 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2020-07-28 16:14:27,082] {logging_mixin.py:112} INFO - [2020-07-28 16:14:27,082] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.94:4042
[2020-07-28 16:14:35,413] {logging_mixin.py:112} INFO - [2020-07-28 16:14:35,412] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:35 INFO Executor: Starting executor ID driver on host 192.168.43.94
[2020-07-28 16:14:36,645] {logging_mixin.py:112} INFO - [2020-07-28 16:14:36,644] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38427.
[2020-07-28 16:14:36,647] {logging_mixin.py:112} INFO - [2020-07-28 16:14:36,646] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:36 INFO NettyBlockTransferService: Server created on 192.168.43.94:38427
[2020-07-28 16:14:36,690] {logging_mixin.py:112} INFO - [2020-07-28 16:14:36,689] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2020-07-28 16:14:37,313] {logging_mixin.py:112} INFO - [2020-07-28 16:14:37,313] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.94, 38427, None)
[2020-07-28 16:14:37,322] {logging_mixin.py:112} INFO - [2020-07-28 16:14:37,322] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.94:38427 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.43.94, 38427, None)
[2020-07-28 16:14:37,643] {logging_mixin.py:112} INFO - [2020-07-28 16:14:37,643] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.94, 38427, None)
[2020-07-28 16:14:37,812] {logging_mixin.py:112} INFO - [2020-07-28 16:14:37,811] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.94, 38427, None)
[2020-07-28 16:14:47,850] {logging_mixin.py:112} INFO - [2020-07-28 16:14:47,850] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:47 INFO AsyncEventQueue: Process of event SparkListenerExecutorAdded(1595933075474,driver,org.apache.spark.scheduler.cluster.ExecutorInfo@4eff259e) by listener HeartbeatReceiver took 1.86197055s.
[2020-07-28 16:14:49,897] {logging_mixin.py:112} INFO - [2020-07-28 16:14:49,896] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:49 INFO AsyncEventQueue: Process of event SparkListenerExecutorAdded(1595933075474,driver,org.apache.spark.scheduler.cluster.ExecutorInfo@4eff259e) by listener AppStatusListener took 4.383552275s.
[2020-07-28 16:14:56,964] {logging_mixin.py:112} INFO - [2020-07-28 16:14:56,963] {spark_submit_hook.py:479} INFO - 20/07/28 16:14:56 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1595933077318,BlockManagerId(driver, 192.168.43.94, 38427, None),384093388,Some(384093388),Some(0)) by listener AppStatusListener took 7.065478867s.
[2020-07-28 16:15:00,010] {logging_mixin.py:112} INFO - [2020-07-28 16:15:00,009] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:00 INFO AsyncEventQueue: Process of event SparkListenerEnvironmentUpdate(Map(Spark Properties -> WrappedArray((spark.app.id,local-1595933069978), (spark.app.name,Spark_Test_Python), (spark.driver.host,192.168.43.94), (spark.driver.port,34533), (spark.executor.id,driver), (spark.master,local), (spark.rdd.compress,True), (spark.scheduler.mode,FIFO), (spark.serializer.objectStreamReset,100), (spark.submit.deployMode,client), (spark.submit.pyFiles,)), Classpath Entries -> Vector((/opt/spark/conf/,System Classpath), (/opt/spark/jars/HikariCP-2.5.1.jar,System Classpath), (/opt/spark/jars/JLargeArrays-1.5.jar,System Classpath), (/opt/spark/jars/JTransforms-3.1.jar,System Classpath), (/opt/spark/jars/RoaringBitmap-0.7.45.jar,System Classpath), (/opt/spark/jars/ST4-4.0.4.jar,System Classpath), (/opt/spark/jars/activation-1.1.1.jar,System Classpath), (/opt/spark/jars/aircompressor-0.10.jar,System Classpath), (/opt/spark/jars/algebra_2.12-2.0.0-M2.jar,System Classpath), (/opt/spark/jars/antlr-runtime-3.5.2.jar,System Classpath), (/opt/spark/jars/antlr4-runtime-4.7.1.jar,System Classpath), (/opt/spark/jars/aopalliance-1.0.jar,System Classpath), (/opt/spark/jars/aopalliance-repackaged-2.6.1.jar,System Classpath), (/opt/spark/jars/apacheds-i18n-2.0.0-M15.jar,System Classpath), (/opt/spark/jars/apacheds-kerberos-codec-2.0.0-M15.jar,System Classpath), (/opt/spark/jars/api-asn1-api-1.0.0-M20.jar,System Classpath), (/opt/spark/jars/api-util-1.0.0-M20.jar,System Classpath), (/opt/spark/jars/arpack_combined_all-0.1.jar,System Classpath), (/opt/spark/jars/arrow-format-0.15.1.jar,System Classpath), (/opt/spark/jars/arrow-memory-0.15.1.jar,System Classpath), (/opt/spark/jars/arrow-vector-0.15.1.jar,System Classpath), (/opt/spark/jars/audience-annotations-0.5.0.jar,System Classpath), (/opt/spark/jars/automaton-1.11-8.jar,System Classpath), (/opt/spark/jars/avro-1.8.2.jar,System Classpath), (/opt/spark/jars/avro-ipc-1.8.2.jar,System Classpath), (/opt/spark/jars/avro-mapred-1.8.2-hadoop2.jar,System Classpath), (/opt/spark/jars/bonecp-0.8.0.RELEASE.jar,System Classpath), (/opt/spark/jars/breeze-macros_2.12-1.0.jar,System Classpath), (/opt/spark/jars/breeze_2.12-1.0.jar,System Classpath), (/opt/spark/jars/cats-kernel_2.12-2.0.0-M4.jar,System Classpath), (/opt/spark/jars/chill-java-0.9.5.jar,System Classpath), (/opt/spark/jars/chill_2.12-0.9.5.jar,System Classpath), (/opt/spark/jars/commons-beanutils-1.9.4.jar,System Classpath), (/opt/spark/jars/commons-cli-1.2.jar,System Classpath), (/opt/spark/jars/commons-codec-1.10.jar,System Classpath), (/opt/spark/jars/commons-collections-3.2.2.jar,System Classpath), (/opt/spark/jars/commons-compiler-3.0.16.jar,System Classpath), (/opt/spark/jars/commons-compress-1.8.1.jar,System Classpath), (/opt/spark/jars/commons-configuration-1.6.jar,System Classpath), (/opt/spark/jars/commons-crypto-1.0.0.jar,System Classpath), (/opt/spark/jars/commons-dbcp-1.4.jar,System Classpath), (/opt/spark/jars/commons-digester-1.8.jar,System Classpath), (/opt/spark/jars/commons-httpclient-3.1.jar,System Classpath), (/opt/spark/jars/commons-io-2.4.jar,System Classpath), (/opt/spark/jars/commons-lang-2.6.jar,System Classpath), (/opt/spark/jars/commons-lang3-3.9.jar,System Classpath), (/opt/spark/jars/commons-logging-1.1.3.jar,System Classpath), (/opt/spark/jars/commons-math3-3.4.1.jar,System Classpath), (/opt/spark/jars/commons-net-3.1.jar,System Classpath), (/opt/spark/jars/commons-pool-1.5.4.jar,System Classpath), (/opt/spark/jars/commons-text-1.6.jar,System Classpath), (/opt/spark/jars/compress-lzf-1.0.3.jar,System Classpath), (/opt/spark/jars/core-1.1.2.jar,System Classpath), (/opt/spark/jars/curator-client-2.7.1.jar,System Classpath), (/opt/spark/jars/curator-framework-2.7.1.jar,System Classpath), (/opt/spark/jars/curator-recipes-2.7.1.jar,System Classpath), (/opt/spark/jars/datanucleus-api-jdo-4.2.4.jar,System Classpath), (/opt/spark/jars/datanucleus-core-4.1.17.jar,System Classpath), (/opt/spark/jars/datanucleus-rdbms-4.1.19.jar,System Classpath), (/opt/spark/jars/derby-10.12.1.1.jar,System Classpath), (/opt/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar,System Classpath), (/opt/spark/jars/flatbuffers-java-1.9.0.jar,System Classpath), (/opt/spark/jars/generex-1.0.2.jar,System Classpath), (/opt/spark/jars/gson-2.2.4.jar,System Classpath), (/opt/spark/jars/guava-14.0.1.jar,System Classpath), (/opt/spark/jars/guice-3.0.jar,System Classpath), (/opt/spark/jars/guice-servlet-3.0.jar,System Classpath), (/opt/spark/jars/hadoop-annotations-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-auth-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-client-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-common-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-hdfs-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-mapreduce-client-app-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-mapreduce-client-common-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-mapreduce-client-core-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-yarn-api-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-yarn-client-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-yarn-common-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-yarn-server-common-2.7.4.jar,System Classpath), (/opt/spark/jars/hadoop-yarn-server-web-proxy-2.7.4.jar,System Classpath), (/opt/spark/jars/hive-beeline-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-cli-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-common-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-exec-2.3.7-core.jar,System Classpath), (/opt/spark/jars/hive-jdbc-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-llap-common-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-metastore-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-serde-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-shims-0.23-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-shims-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-shims-common-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-shims-scheduler-2.3.7.jar,System Classpath), (/opt/spark/jars/hive-storage-api-2.7.1.jar,System Classpath), (/opt/spark/jars/hive-vector-code-gen-2.3.7.jar,System Classpath), (/opt/spark/jars/hk2-api-2.6.1.jar,System Classpath), (/opt/spark/jars/hk2-locator-2.6.1.jar,System Classpath), (/opt/spark/jars/hk2-utils-2.6.1.jar,System Classpath), (/opt/spark/jars/htrace-core-3.1.0-incubating.jar,System Classpath), (/opt/spark/jars/httpclient-4.5.6.jar,System Classpath), (/opt/spark/jars/httpcore-4.4.12.jar,System Classpath), (/opt/spark/jars/istack-commons-runtime-3.0.8.jar,System Classpath), (/opt/spark/jars/ivy-2.4.0.jar,System Classpath), (/opt/spark/jars/jackson-annotations-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-core-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-core-asl-1.9.13.jar,System Classpath), (/opt/spark/jars/jackson-databind-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-dataformat-yaml-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-datatype-jsr310-2.10.3.jar,System Classpath), (/opt/spark/jars/jackson-jaxrs-1.9.13.jar,System Classpath), (/opt/spark/jars/jackson-mapper-asl-1.9.13.jar,System Classpath), (/opt/spark/jars/jackson-module-jaxb-annotations-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-module-paranamer-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-module-scala_2.12-2.10.0.jar,System Classpath), (/opt/spark/jars/jackson-xc-1.9.13.jar,System Classpath), (/opt/spark/jars/jakarta.activation-api-1.2.1.jar,System Classpath), (/opt/spark/jars/jakarta.annotation-api-1.3.5.jar,System Classpath), (/opt/spark/jars/jakarta.inject-2.6.1.jar,System Classpath), (/opt/spark/jars/jakarta.validation-api-2.0.2.jar,System Classpath), (/opt/spark/jars/jakarta.ws.rs-api-2.1.6.jar,System Classpath), (/opt/spark/jars/jakarta.xml.bind-api-2.3.2.jar,System Classpath), (/opt/spark/jars/janino-3.0.16.jar,System Classpath), (/opt/spark/jars/javassist-3.25.0-GA.jar,System Classpath), (/opt/spark/jars/javax.inject-1.jar,System Classpath), (/opt/spark/jars/javax.jdo-3.2.0-m3.jar,System Classpath), (/opt/spark/jars/javax.servlet-api-3.1.0.jar,System Classpath), (/opt/spark/jars/javolution-5.5.1.jar,System Classpath), (/opt/spark/jars/jaxb-api-2.2.2.jar,System Classpath), (/opt/spark/jars/jaxb-runtime-2.3.2.jar,System Classpath), (/opt/spark/jars/jcl-over-slf4j-1.7.30.jar,System Classpath), (/opt/spark/jars/jdo-api-3.0.1.jar,System Classpath), (/opt/spark/jars/jersey-client-2.30.jar,System Classpath), (/opt/spark/jars/jersey-common-2.30.jar,System Classpath), (/opt/spark/jars/jersey-container-servlet-2.30.jar,System Classpath), (/opt/spark/jars/jersey-container-servlet-core-2.30.jar,System Classpath), (/opt/spark/jars/jersey-hk2-2.30.jar,System Classpath), (/opt/spark/jars/jersey-media-jaxb-2.30.jar,System Classpath), (/opt/spark/jars/jersey-server-2.30.jar,System Classpath), (/opt/spark/jars/jetty-6.1.26.jar,System Classpath), (/opt/spark/jars/jetty-sslengine-6.1.26.jar,System Classpath), (/opt/spark/jars/jetty-util-6.1.26.jar,System Classpath), (/opt/spark/jars/jline-2.14.6.jar,System Classpath), (/opt/spark/jars/joda-time-2.10.5.jar,System Classpath), (/opt/spark/jars/jodd-core-3.5.2.jar,System Classpath), (/opt/spark/jars/jpam-1.1.jar,System Classpath), (/opt/spark/jars/json-1.8.jar,System Classpath), (/opt/spark/jars/json4s-ast_2.12-3.6.6.jar,System Classpath), (/opt/spark/jars/json4s-core_2.12-3.6.6.jar,System Classpath), (/opt/spark/jars/json4s-jackson_2.12-3.6.6.jar,System Classpath), (/opt/spark/jars/json4s-scalap_2.12-3.6.6.jar,System Classpath), (/opt/spark/jars/jsp-api-2.1.jar,System Classpath), (/opt/spark/jars/jsr305-3.0.0.jar,System Classpath), (/opt/spark/jars/jta-1.1.jar,System Classpath), (/opt/spark/jars/jul-to-slf4j-1.7.30.jar,System Classpath), (/opt/spark/jars/kryo-shaded-4.0.2.jar,System Classpath), (/opt/spark/jars/kubernetes-client-4.9.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-4.9.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-common-4.9.2.jar,System Classpath), (/opt/spark/jars/leveldbjni-all-1.8.jar,System Classpath), (/opt/spark/jars/libfb303-0.9.3.jar,System Classpath), (/opt/spark/jars/libthrift-0.12.0.jar,System Classpath), (/opt/spark/jars/log4j-1.2.17.jar,System Classpath), (/opt/spark/jars/logging-interceptor-3.12.6.jar,System Classpath), (/opt/spark/jars/lz4-java-1.7.1.jar,System Classpath), (/opt/spark/jars/machinist_2.12-0.6.8.jar,System Classpath), (/opt/spark/jars/macro-compat_2.12-1.1.1.jar,System Classpath), (/opt/spark/jars/mesos-1.4.0-shaded-protobuf.jar,System Classpath), (/opt/spark/jars/metrics-core-4.1.1.jar,System Classpath), (/opt/spark/jars/metrics-graphite-4.1.1.jar,System Classpath), (/opt/spark/jars/metrics-jmx-4.1.1.jar,System Classpath), (/opt/spark/jars/metrics-json-4.1.1.jar,System Classpath), (/opt/spark/jars/metrics-jvm-4.1.1.jar,System Classpath), (/opt/spark/jars/minlog-1.3.0.jar,System Classpath), (/opt/spark/jars/netty-all-4.1.47.Final.jar,System Classpath), (/opt/spark/jars/objenesis-2.5.1.jar,System Classpath), (/opt/spark/jars/okhttp-3.12.6.jar,System Classpath), (/opt/spark/jars/okio-1.15.0.jar,System Classpath), (/opt/spark/jars/opencsv-2.3.jar,System Classpath), (/opt/spark/jars/orc-core-1.5.10.jar,System Classpath), (/opt/spark/jars/orc-mapreduce-1.5.10.jar,System Classpath), (/opt/spark/jars/orc-shims-1.5.10.jar,System Classpath), (/opt/spark/jars/oro-2.0.8.jar,System Classpath), (/opt/spark/jars/osgi-resource-locator-1.0.3.jar,System Classpath), (/opt/spark/jars/paranamer-2.8.jar,System Classpath), (/opt/spark/jars/parquet-column-1.10.1.jar,System Classpath), (/opt/spark/jars/parquet-common-1.10.1.jar,System Classpath), (/opt/spark/jars/parquet-encoding-1.10.1.jar,System Classpath), (/opt/spark/jars/parquet-format-2.4.0.jar,System Classpath), (/opt/spark/jars/parquet-hadoop-1.10.1.jar,System Classpath), (/opt/spark/jars/parquet-jackson-1.10.1.jar,System Classpath), (/opt/spark/jars/protobuf-java-2.5.0.jar,System Classpath), (/opt/spark/jars/py4j-0.10.9.jar,System Classpath), (/opt/spark/jars/pyrolite-4.30.jar,System Classpath), (/opt/spark/jars/scala-collection-compat_2.12-2.1.1.jar,System Classpath), (/opt/spark/jars/scala-compiler-2.12.10.jar,System Classpath), (/opt/spark/jars/scala-library-2.12.10.jar,System Classpath), (/opt/spark/jars/scala-parser-combinators_2.12-1.1.2.jar,System Classpath), (/opt/spark/jars/scala-reflect-2.12.10.jar,System Classpath), (/opt/spark/jars/scala-xml_2.12-1.2.0.jar,System Classpath), (/opt/spark/jars/shapeless_2.12-2.3.3.jar,System Classpath), (/opt/spark/jars/shims-0.7.45.jar,System Classpath), (/opt/spark/jars/slf4j-api-1.7.30.jar,System Classpath), (/opt/spark/jars/slf4j-log4j12-1.7.30.jar,System Classpath), (/opt/spark/jars/snakeyaml-1.24.jar,System Classpath), (/opt/spark/jars/snappy-java-1.1.7.5.jar,System Classpath), (/opt/spark/jars/spark-catalyst_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-core_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-graphx_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-hive-thriftserver_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-hive_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-kubernetes_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-kvstore_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-launcher_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-mesos_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-mllib-local_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-mllib_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-network-common_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-network-shuffle_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-repl_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-sketch_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-sql_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-streaming_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-tags_2.12-3.0.0-tests.jar,System Classpath), (/opt/spark/jars/spark-tags_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spark-yarn_2.12-3.0.0.jar,System Classpath), (/opt/spark/jars/spire-macros_2.12-0.17.0-M1.jar,System Classpath), (/opt/spark/jars/spire-platform_2.12-0.17.0-M1.jar,System Classpath), (/opt/spark/jars/spire-util_2.12-0.17.0-M1.jar,System Classpath), (/opt/spark/jars/spire_2.12-0.17.0-M1.jar,System Classpath), (/opt/spark/jars/stax-api-1.0-2.jar,System Classpath), (/opt/spark/jars/stax-api-1.0.1.jar,System Classpath), (/opt/spark/jars/stream-2.9.6.jar,System Classpath), (/opt/spark/jars/super-csv-2.2.0.jar,System Classpath), (/opt/spark/jars/threeten-extra-1.5.0.jar,System Classpath), (/opt/spark/jars/transaction-api-1.1.jar,System Classpath), (/opt/spark/jars/univocity-parsers-2.8.3.jar,System Classpath), (/opt/spark/jars/velocity-1.5.jar,System Classpath), (/opt/spark/jars/xbean-asm7-shaded-4.15.jar,System Classpath), (/opt/spark/jars/xercesImpl-2.12.0.jar,System Classpath), (/opt/spark/jars/xml-apis-1.4.01.jar,System Classpath), (/opt/spark/jars/xmlenc-0.52.jar,System Classpath), (/opt/spark/jars/xz-1.5.jar,System Classpath), (/opt/spark/jars/zjsonpatch-0.3.0.jar,System Classpath), (/opt/spark/jars/zookeeper-3.4.14.jar,System Classpath), (/opt/spark/jars/zstd-jni-1.4.4-3.jar,System Classpath)), Hadoop Properties -> List((dfs.ha.fencing.ssh.connect-timeout,30000), (file.blocksize,67108864), (file.bytes-per-checksum,512), (file.client-write-packet-size,65536), (file.replication,1), (file.stream-buffer-size,4096), (fs.AbstractFileSystem.file.impl,org.apache.hadoop.fs.local.LocalFs), (fs.AbstractFileSystem.ftp.impl,org.apache.hadoop.fs.ftp.FtpFs), (fs.AbstractFileSystem.har.impl,org.apache.hadoop.fs.HarFs), (fs.AbstractFileSystem.hdfs.impl,org.apache.hadoop.fs.Hdfs), (fs.AbstractFileSystem.viewfs.impl,org.apache.hadoop.fs.viewfs.ViewFs), (fs.automatic.close,true), (fs.client.resolve.remote.symlinks,true), (fs.defaultFS,file:///), (fs.df.interval,60000), (fs.du.interval,600000), (fs.ftp.host,0.0.0.0), (fs.ftp.host.port,21), (fs.har.impl.disable.cache,true), (fs.permissions.umask-mode,022), (fs.s3.block.size,67108864), (fs.s3.buffer.dir,${hadoop.tmp.dir}/s3), (fs.s3.maxRetries,4), (fs.s3.sleepTimeSeconds,10), (fs.s3a.attempts.maximum,10), (fs.s3a.buffer.dir,${hadoop.tmp.dir}/s3a), (fs.s3a.connection.establish.timeout,5000), (fs.s3a.connection.maximum,15), (fs.s3a.connection.ssl.enabled,true), (fs.s3a.connection.timeout,50000), (fs.s3a.fast.buffer.size,1048576), (fs.s3a.fast.upload,false), (fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (fs.s3a.max.total.tasks,1000), (fs.s3a.multipart.purge,false), (fs.s3a.multipart.purge.age,86400), (fs.s3a.multipart.size,104857600), (fs.s3a.multipart.threshold,2147483647), (fs.s3a.paging.maximum,5000), (fs.s3a.threads.core,15), (fs.s3a.threads.keepalivetime,60), (fs.s3a.threads.max,256), (fs.s3n.block.size,67108864), (fs.s3n.multipart.copy.block.size,5368709120), (fs.s3n.multipart.uploads.block.size,67108864), (fs.s3n.multipart.uploads.enabled,false), (fs.swift.impl,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem), (fs.trash.checkpoint.interval,0), (fs.trash.interval,0), (ftp.blocksize,67108864), (ftp.bytes-per-checksum,512), (ftp.client-write-packet-size,65536), (ftp.replication,3), (ftp.stream-buffer-size,4096), (ha.failover-controller.cli-check.rpc-timeout.ms,20000), (ha.failover-controller.graceful-fence.connection.retries,1), (ha.failover-controller.graceful-fence.rpc-timeout.ms,5000), (ha.failover-controller.new-active.rpc-timeout.ms,60000), (ha.health-monitor.check-interval.ms,1000), (ha.health-monitor.connect-retry-interval.ms,1000), (ha.health-monitor.rpc-timeout.ms,45000), (ha.health-monitor.sleep-after-disconnect.ms,1000), (ha.zookeeper.acl,world:anyone:rwcda), (ha.zookeeper.parent-znode,/hadoop-ha), (ha.zookeeper.session-timeout.ms,5000), (hadoop.common.configuration.version,0.23.0), (hadoop.http.authentication.kerberos.keytab,${user.home}/hadoop.keytab), (hadoop.http.authentication.kerberos.principal,HTTP/_HOST@LOCALHOST), (hadoop.http.authentication.signature.secret.file,${user.home}/hadoop-http-auth-signature-secret), (hadoop.http.authentication.simple.anonymous.allowed,true), (hadoop.http.authentication.token.validity,36000), (hadoop.http.authentication.type,simple), (hadoop.http.cross-origin.allowed-headers,X-Requested-With,Content-Type,Accept,Origin), (hadoop.http.cross-origin.allowed-methods,GET,POST,HEAD), (hadoop.http.cross-origin.allowed-origins,*), (hadoop.http.cross-origin.enabled,false), (hadoop.http.cross-origin.max-age,1800), (hadoop.http.filter.initializers,org.apache.hadoop.http.lib.StaticUserWebFilter), (hadoop.http.staticuser.user,dr.who), (hadoop.jetty.logs.serve.aliases,true), (hadoop.kerberos.kinit.command,kinit), (hadoop.registry.jaas.context,Client), (hadoop.registry.rm.enabled,false), (hadoop.registry.secure,false), (hadoop.registry.system.acls,sasl:yarn@, sasl:mapred@, sasl:hdfs@), (hadoop.registry.zk.connection.timeout.ms,15000), (hadoop.registry.zk.quorum,localhost:2181), (hadoop.registry.zk.retry.ceiling.ms,60000), (hadoop.registry.zk.retry.interval.ms,1000), (hadoop.registry.zk.retry.times,5), (hadoop.registry.zk.root,/registry), (hadoop.registry.zk.session.timeout.ms,60000), (hadoop.rpc.protection,authentication), (hadoop.rpc.socket.factory.class.default,org.apache.hadoop.net.StandardSocketFactory), (hadoop.security.authentication,simple), (hadoop.security.authorization,false), (hadoop.security.crypto.buffer.size,8192), (hadoop.security.crypto.cipher.suite,AES/CTR/NoPadding), (hadoop.security.crypto.codec.classes.aes.ctr.nopadding,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec), (hadoop.security.group.mapping,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback), (hadoop.security.group.mapping.ldap.directory.search.timeout,10000), (hadoop.security.group.mapping.ldap.search.attr.group.name,cn), (hadoop.security.group.mapping.ldap.search.attr.member,member), (hadoop.security.group.mapping.ldap.search.filter.group,(objectClass=group)), (hadoop.security.group.mapping.ldap.search.filter.user,(&(objectClass=user)(sAMAccountName={0}))), (hadoop.security.group.mapping.ldap.ssl,false), (hadoop.security.groups.cache.secs,300), (hadoop.security.groups.cache.warn.after.ms,5000), (hadoop.security.groups.negative-cache.secs,30), (hadoop.security.instrumentation.requires.admin,false), (hadoop.security.java.secure.random.algorithm,SHA1PRNG), (hadoop.security.kms.client.authentication.retry-count,1), (hadoop.security.kms.client.encrypted.key.cache.expiry,43200000), (hadoop.security.kms.client.encrypted.key.cache.low-watermark,0.3f), (hadoop.security.kms.client.encrypted.key.cache.num.refill.threads,2), (hadoop.security.kms.client.encrypted.key.cache.size,500), (hadoop.security.random.device.file.path,/dev/urandom), (hadoop.security.sensitive-config-keys,password$,fs.s3.*[Ss]ecret.?[Kk]ey,fs.azure.account.key.*,dfs.webhdfs.oauth2.[a-z]+.token,hadoop.security.sensitive-config-keys), (hadoop.security.uid.cache.secs,14400), (hadoop.ssl.client.conf,ssl-client.xml), (hadoop.ssl.enabled,false), (hadoop.ssl.enabled.protocols,TLSv1), (hadoop.ssl.hostname.verifier,DEFAULT), (hadoop.ssl.keystores.factory.class,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory), (hadoop.ssl.require.client.cert,false), (hadoop.ssl.server.conf,ssl-server.xml), (hadoop.tmp.dir,/tmp/hadoop-${user.name}), (hadoop.user.group.static.mapping.overrides,dr.who=;), (hadoop.util.hash.type,murmur), (hadoop.work.around.non.threadsafe.getpwuid,false), (io.bytes.per.checksum,512), (io.compression.codec.bzip2.library,system-native), (io.file.buffer.size,65536), (io.map.index.interval,128), (io.map.index.skip,0), (io.mapfile.bloom.error.rate,0.005), (io.mapfile.bloom.size,1048576), (io.native.lib.available,true), (io.seqfile.compress.blocksize,1000000), (io.seqfile.lazydecompress,true), (io.seqfile.local.dir,${hadoop.tmp.dir}/io/local), (io.seqfile.sorter.recordlimit,1000000), (io.serializations,org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization), (io.skip.checksum.errors,false), (ipc.client.connect.max.retries,10), (ipc.client.connect.max.retries.on.timeouts,45), (ipc.client.connect.retry.interval,1000), (ipc.client.connect.timeout,20000), (ipc.client.connection.maxidletime,10000), (ipc.client.fallback-to-simple-auth-allowed,false), (ipc.client.idlethreshold,4000), (ipc.client.kill.max,10), (ipc.client.ping,true), (ipc.client.rpc-timeout.ms,0), (ipc.maximum.data.length,67108864), (ipc.ping.interval,60000), (ipc.server.listen.queue.size,128), (ipc.server.log.slow.rpc,false), (ipc.server.max.connections,0), (map.sort.class,org.apache.hadoop.util.QuickSort), (mapred.child.java.opts,-Xmx200m), (mapreduce.am.max-attempts,2), (mapreduce.app-submission.cross-platform,false), (mapreduce.client.completion.pollinterval,5000), (mapreduce.client.output.filter,FAILED), (mapreduce.client.progressmonitor.pollinterval,1000), (mapreduce.client.submit.file.replication,10), (mapreduce.cluster.acls.enabled,false), (mapreduce.cluster.local.dir,${hadoop.tmp.dir}/mapred/local), (mapreduce.cluster.temp.dir,${hadoop.tmp.dir}/mapred/temp), (mapreduce.fileoutputcommitter.algorithm.version,1), (mapreduce.framework.name,local), (mapreduce.ifile.readahead,true), (mapreduce.ifile.readahead.bytes,4194304), (mapreduce.input.fileinputformat.list-status.num-threads,1), (mapreduce.input.fileinputformat.split.minsize,0), (mapreduce.input.lineinputformat.linespermap,1), (mapreduce.job.acl-modify-job, ), (mapreduce.job.acl-view-job, ), (mapreduce.job.classloader,false), (mapreduce.job.committer.setup.cleanup.needed,true), (mapreduce.job.complete.cancel.delegation.tokens,true), (mapreduce.job.counters.max,120), (mapreduce.job.emit-timeline-data,false), (mapreduce.job.end-notification.max.attempts,5), (mapreduce.job.end-notification.max.retry.interval,5000), (mapreduce.job.end-notification.retry.attempts,0), (mapreduce.job.end-notification.retry.interval,1000), (mapreduce.job.hdfs-servers,${fs.defaultFS}), (mapreduce.job.jvm.numtasks,1), (mapreduce.job.map.output.collector.class,org.apache.hadoop.mapred.MapTask$MapOutputBuffer), (mapreduce.job.maps,2), (mapreduce.job.max.split.locations,10), (mapreduce.job.maxtaskfailures.per.tracker,3), (mapreduce.job.queuename,default), (mapreduce.job.reduce.shuffle.consumer.plugin.class,org.apache.hadoop.mapreduce.task.reduce.Shuffle), (mapreduce.job.reduce.slowstart.completedmaps,0.05), (mapreduce.job.reducer.preempt.delay.sec,0), (mapreduce.job.reducer.unconditional-preempt.delay.sec,300), (mapreduce.job.reduces,1), (mapreduce.job.running.map.limit,0), (mapreduce.job.running.reduce.limit,0), (mapreduce.job.speculative.minimum-allowed-tasks,10), (mapreduce.job.speculative.retry-after-no-speculate,1000), (mapreduce.job.speculative.retry-after-speculate,15000), (mapreduce.job.speculative.slowtaskthreshold,1.0), (mapreduce.job.speculative.speculative-cap-running-tasks,0.1), (mapreduce.job.speculative.speculative-cap-total-tasks,0.01), (mapreduce.job.split.metainfo.maxsize,10000000), (mapreduce.job.token.tracking.ids.enabled,false), (mapreduce.job.ubertask.enable,false), (mapreduce.job.ubertask.maxmaps,9), (mapreduce.job.ubertask.maxreduces,1), (mapreduce.job.userlog.retain.hours,24), (mapreduce.jobhistory.address,0.0.0.0:10020), (mapreduce.jobhistory.admin.acl,*), (mapreduce.jobhistory.admin.address,0.0.0.0:10033), (mapreduce.jobhistory.cleaner.enable,true), (mapreduce.jobhistory.cleaner.interval-ms,86400000), (mapreduce.jobhistory.client.thread-count,10), (mapreduce.jobhistory.datestring.cache.size,200000), (mapreduce.jobhistory.done-dir,${yarn.app.mapreduce.am.staging-dir}/history/done), (mapreduce.jobhistory.http.policy,HTTP_ONLY), (mapreduce.jobhistory.intermediate-done-dir,${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate), (mapreduce.jobhistory.joblist.cache.size,20000), (mapreduce.jobhistory.keytab,/etc/security/keytab/jhs.service.keytab), (mapreduce.jobhistory.loadedjobs.cache.size,5), (mapreduce.jobhistory.max-age-ms,604800000), (mapreduce.jobhistory.minicluster.fixed.ports,false), (mapreduce.jobhistory.move.interval-ms,180000), (mapreduce.jobhistory.move.thread-count,3), (mapreduce.jobhistory.principal,jhs/_HOST@REALM.TLD), (mapreduce.jobhistory.recovery.enable,false), (mapreduce.jobhistory.recovery.store.class,org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService), (mapreduce.jobhistory.recovery.store.fs.uri,${hadoop.tmp.dir}/mapred/history/recoverystore), (mapreduce.jobhistory.recovery.store.leveldb.path,${hadoop.tmp.dir}/mapred/history/recoverystore), (mapreduce.jobhistory.webapp.address,0.0.0.0:19888), (mapreduce.jobtracker.address,local), (mapreduce.jobtracker.expire.trackers.interval,600000), (mapreduce.jobtracker.handler.count,10), (mapreduce.jobtracker.heartbeats.in.second,100), (mapreduce.jobtracker.http.address,0.0.0.0:50030), (mapreduce.jobtracker.instrumentation,org.apache.hadoop.mapred.JobTrackerMetricsInst), (mapreduce.jobtracker.jobhistory.block.size,3145728), (mapreduce.jobtracker.jobhistory.lru.cache.size,5), (mapreduce.jobtracker.jobhistory.task.numberprogresssplits,12), (mapreduce.jobtracker.maxtasks.perjob,-1), (mapreduce.jobtracker.persist.jobstatus.active,true), (mapreduce.jobtracker.persist.jobstatus.dir,/jobtracker/jobsInfo), (mapreduce.jobtracker.persist.jobstatus.hours,1), (mapreduce.jobtracker.restart.recover,false), (mapreduce.jobtracker.retiredjobs.cache.size,1000), (mapreduce.jobtracker.staging.root.dir,${hadoop.tmp.dir}/mapred/staging), (mapreduce.jobtracker.system.dir,${hadoop.tmp.dir}/mapred/system), (mapreduce.jobtracker.taskcache.levels,2), (mapreduce.jobtracker.taskscheduler,org.apache.hadoop.mapred.JobQueueTaskScheduler), (mapreduce.jobtracker.tasktracker.maxblacklists,4), (mapreduce.local.clientfactory.class.name,org.apache.hadoop.mapred.LocalClientFactory), (mapreduce.map.cpu.vcores,1), (mapreduce.map.log.level,INFO), (mapreduce.map.maxattempts,4), (mapreduce.map.memory.mb,1024), (mapreduce.map.output.compress,false), (mapreduce.map.output.compress.codec,org.apache.hadoop.io.compress.DefaultCodec), (mapreduce.map.skip.maxrecords,0), (mapreduce.map.skip.proc.count.autoincr,true), (mapreduce.map.sort.spill.percent,0.80), (mapreduce.map.speculative,true), (mapreduce.output.fileoutputformat.compress,false), (mapreduce.output.fileoutputformat.compress.codec,org.apache.hadoop.io.compress.DefaultCodec), (mapreduce.output.fileoutputformat.compress.type,RECORD), (mapreduce.reduce.cpu.vcores,1), (mapreduce.reduce.input.buffer.percent,0.0), (mapreduce.reduce.log.level,INFO), (mapreduce.reduce.markreset.buffer.percent,0.0), (mapreduce.reduce.maxattempts,4), (mapreduce.reduce.memory.mb,1024), (mapreduce.reduce.merge.inmem.threshold,1000), (mapreduce.reduce.shuffle.connect.timeout,180000), (mapreduce.reduce.shuffle.fetch.retry.enabled,${yarn.nodemanager.recovery.enabled}), (mapreduce.reduce.shuffle.fetch.retry.interval-ms,1000), (mapreduce.reduce.shuffle.fetch.retry.timeout-ms,30000), (mapreduce.reduce.shuffle.input.buffer.percent,0.70), (mapreduce.reduce.shuffle.memory.limit.percent,0.25), (mapreduce.reduce.shuffle.merge.percent,0.66), (mapreduce.reduce.shuffle.parallelcopies,5), (mapreduce.reduce.shuffle.read.timeout,180000), (mapreduce.reduce.shuffle.retry-delay.max.ms,60000), (mapreduce.reduce.skip.maxgroups,0), (mapreduce.reduce.skip.proc.count.autoincr,true), (mapreduce.reduce.speculative,true), (mapreduce.shuffle.connection-keep-alive.enable,false), (mapreduce.shuffle.connection-keep-alive.timeout,5), (mapreduce.shuffle.listen.queue.size,128), (mapreduce.shuffle.max.connections,0), (mapreduce.shuffle.max.threads,0), (mapreduce.shuffle.port,13562), (mapreduce.shuffle.ssl.enabled,false), (mapreduce.shuffle.ssl.file.buffer.size,65536), (mapreduce.shuffle.transfer.buffer.size,131072), (mapreduce.task.combine.progress.records,10000), (mapreduce.task.files.preserve.failedtasks,false), (mapreduce.task.io.sort.factor,10), (mapreduce.task.io.sort.mb,100), (mapreduce.task.merge.progress.records,10000), (mapreduce.task.profile,false), (mapreduce.task.profile.map.params,${mapreduce.task.profile.params}), (mapreduce.task.profile.maps,0-2), (mapreduce.task.profile.params,-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s), (mapreduce.task.profile.reduce.params,${mapreduce.task.profile.params}), (mapreduce.task.profile.reduces,0-2), (mapreduce.task.skip.start.attempts,2), (mapreduce.task.timeout,600000), (mapreduce.task.userlog.limit.kb,0), (mapreduce.tasktracker.dns.interface,default), (mapreduce.tasktracker.dns.nameserver,default), (mapreduce.tasktracker.healthchecker.interval,60000), (mapreduce.tasktracker.healthchecker.script.timeout,600000), (mapreduce.tasktracker.http.address,0.0.0.0:50060), (mapreduce.tasktracker.http.threads,40), (mapreduce.tasktracker.indexcache.mb,10), (mapreduce.tasktracker.instrumentation,org.apache.hadoop.mapred.TaskTrackerMetricsInst), (mapreduce.tasktracker.local.dir.minspacekill,0), (mapreduce.tasktracker.local.dir.minspacestart,0), (mapreduce.tasktracker.map.tasks.maximum,2), (mapreduce.tasktracker.outofband.heartbeat,false), (mapreduce.tasktracker.reduce.tasks.maximum,2), (mapreduce.tasktracker.report.address,127.0.0.1:0), (mapreduce.tasktracker.taskcontroller,org.apache.hadoop.mapred.DefaultTaskController), (mapreduce.tasktracker.taskmemorymanager.monitoringinterval,5000), (mapreduce.tasktracker.tasks.sleeptimebeforesigkill,5000), (net.topology.impl,org.apache.hadoop.net.NetworkTopology), (net.topology.node.switch.mapping.impl,org.apache.hadoop.net.ScriptBasedMapping), (net.topology.script.number.args,100), (nfs.exports.allowed.hosts,* rw), (rpc.metrics.quantile.enable,false), (s3.blocksize,67108864), (s3.bytes-per-checksum,512), (s3.client-write-packet-size,65536), (s3.replication,3), (s3.stream-buffer-size,4096), (s3native.blocksize,67108864), (s3native.bytes-per-checksum,512), (s3native.client-write-packet-size,65536), (s3native.replication,3), (s3native.stream-buffer-size,4096), (tfile.fs.input.buffer.size,262144), (tfile.fs.output.buffer.size,262144), (tfile.io.chunk.size,1048576), (yarn.acl.enable,false), (yarn.admin.acl,*), (yarn.am.liveness-monitor.expiry-interval-ms,600000), (yarn.app.mapreduce.am.command-opts,-Xmx1024m), (yarn.app.mapreduce.am.container.log.backups,0), (yarn.app.mapreduce.am.container.log.limit.kb,0), (yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size,10), (yarn.app.mapreduce.am.hard-kill-timeout-ms,10000), (yarn.app.mapreduce.am.job.committer.cancel-timeout,60000), (yarn.app.mapreduce.am.job.committer.commit-window,10000), (yarn.app.mapreduce.am.job.task.listener.thread-count,30), (yarn.app.mapreduce.am.resource.cpu-vcores,1), (yarn.app.mapreduce.am.resource.mb,1536), (yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,1000), (yarn.app.mapreduce.am.staging-dir,/tmp/hadoop-yarn/staging), (yarn.app.mapreduce.client-am.ipc.max-retries,3), (yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,3), (yarn.app.mapreduce.client.job.max-retries,0), (yarn.app.mapreduce.client.job.retry-interval,2000), (yarn.app.mapreduce.client.max-retries,3), (yarn.app.mapreduce.shuffle.log.backups,0), (yarn.app.mapreduce.shuffle.log.limit.kb,0), (yarn.app.mapreduce.shuffle.log.separate,true), (yarn.app.mapreduce.task.container.log.backups,0), (yarn.client.application-client-protocol.poll-interval-ms,200), (yarn.client.failover-proxy-provider,org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider), (yarn.client.failover-retries,0), (yarn.client.failover-retries-on-socket-timeouts,0), (yarn.client.max-cached-nodemanagers-proxies,0), (yarn.client.nodemanager-client-async.thread-pool-max-size,500), (yarn.client.nodemanager-connect.max-wait-ms,180000), (yarn.client.nodemanager-connect.retry-interval-ms,10000), (yarn.dispatcher.drain-events.timeout,300000), (yarn.fail-fast,false), (yarn.http.policy,HTTP_ONLY), (yarn.ipc.rpc.class,org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC), (yarn.log-aggregation-enable,false), (yarn.log-aggregation.retain-check-interval-seconds,-1), (yarn.log-aggregation.retain-seconds,-1), (yarn.nm.liveness-monitor.expiry-interval-ms,600000), (yarn.nodemanager.address,${yarn.nodemanager.hostname}:0), (yarn.nodemanager.admin-env,MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX), (yarn.nodemanager.aux-services.mapreduce_shuffle.class,org.apache.hadoop.mapred.ShuffleHandler), (yarn.nodemanager.container-executor.class,org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor), (yarn.nodemanager.container-manager.thread-count,20), (yarn.nodemanager.container-metrics.unregister-delay-ms,10000), (yarn.nodemanager.container-monitor.interval-ms,3000), (yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled,false), (yarn.nodemanager.delete.debug-delay-sec,0), (yarn.nodemanager.delete.thread-count,4), (yarn.nodemanager.disk-health-checker.interval-ms,120000), (yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,90.0), (yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,0), (yarn.nodemanager.disk-health-checker.min-healthy-disks,0.25), (yarn.nodemanager.docker-container-executor.exec-name,/usr/bin/docker), (yarn.nodemanager.env-whitelist,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME), (yarn.nodemanager.health-checker.interval-ms,600000), (yarn.nodemanager.health-checker.script.timeout-ms,1200000), (yarn.nodemanager.hostname,0.0.0.0), (yarn.nodemanager.keytab,/etc/krb5.keytab), (yarn.nodemanager.linux-container-executor.cgroups.hierarchy,/hadoop-yarn), (yarn.nodemanager.linux-container-executor.cgroups.mount,false), (yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage,false), (yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,true), (yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,nobody), (yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern,^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$), (yarn.nodemanager.linux-container-executor.resources-handler.class,org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler), (yarn.nodemanager.local-cache.max-files-per-directory,8192), (yarn.nodemanager.local-dirs,${hadoop.tmp.dir}/nm-local-dir), (yarn.nodemanager.localizer.address,${yarn.nodemanager.hostname}:8040), (yarn.nodemanager.localizer.cache.cleanup.interval-ms,600000), (yarn.nodemanager.localizer.cache.target-size-mb,10240), (yarn.nodemanager.localizer.client.thread-count,5), (yarn.nodemanager.localizer.fetch.thread-count,4), (yarn.nodemanager.log-aggregation.compression-type,none), (yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds,-1), (yarn.nodemanager.log-dirs,${yarn.log.dir}/userlogs), (yarn.nodemanager.log.retain-seconds,10800), (yarn.nodemanager.pmem-check-enabled,true), (yarn.nodemanager.process-kill-wait.ms,2000), (yarn.nodemanager.recovery.compaction-interval-secs,3600), (yarn.nodemanager.recovery.dir,${hadoop.tmp.dir}/yarn-nm-recovery), (yarn.nodemanager.recovery.enabled,false), (yarn.nodemanager.remote-app-log-dir,/tmp/logs), (yarn.nodemanager.remote-app-log-dir-suffix,logs), (yarn.nodemanager.resource.cpu-vcores,8), (yarn.nodemanager.resource.memory-mb,8192), (yarn.nodemanager.resource.percentage-physical-cpu-limit,100), (yarn.nodemanager.resourcemanager.minimum.version,NONE), (yarn.nodemanager.sleep-delay-before-sigkill.ms,250), (yarn.nodemanager.vmem-check-enabled,true), (yarn.nodemanager.vmem-pmem-ratio,2.1), (yarn.nodemanager.webapp.address,${yarn.nodemanager.hostname}:8042), (yarn.nodemanager.webapp.cross-origin.enabled,false), (yarn.nodemanager.windows-container.cpu-limit.enabled,false), (yarn.nodemanager.windows-container.memory-limit.enabled,false), (yarn.resourcemanager.address,${yarn.resourcemanager.hostname}:8032), (yarn.resourcemanager.admin.address,${yarn.resourcemanager.hostname}:8033), (yarn.resourcemanager.admin.client.thread-count,1), (yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs,86400), (yarn.resourcemanager.am.max-attempts,2), (yarn.resourcemanager.amlauncher.thread-count,50), (yarn.resourcemanager.client.thread-count,50), (yarn.resourcemanager.configuration.provider-class,org.apache.hadoop.yarn.LocalConfigurationProvider), (yarn.resourcemanager.connect.max-wait.ms,900000), (yarn.resourcemanager.connect.retry-interval.ms,30000), (yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,86400), (yarn.resourcemanager.container.liveness-monitor.interval-ms,600000), (yarn.resourcemanager.delayed.delegation-token.removal-interval-ms,30000), (yarn.resourcemanager.fail-fast,${yarn.fail-fast}), (yarn.resourcemanager.fs.state-store.num-retries,0), (yarn.resourcemanager.fs.state-store.retry-interval-ms,1000), (yarn.resourcemanager.fs.state-store.retry-policy-spec,2000, 500), (yarn.resourcemanager.fs.state-store.uri,${hadoop.tmp.dir}/yarn/system/rmstore), (yarn.resourcemanager.ha.automatic-failover.embedded,true), (yarn.resourcemanager.ha.automatic-failover.enabled,true), (yarn.resourcemanager.ha.automatic-failover.zk-base-path,/yarn-leader-election), (yarn.resourcemanager.ha.enabled,false), (yarn.resourcemanager.hostname,0.0.0.0), (yarn.resourcemanager.keytab,/etc/krb5.keytab), (yarn.resourcemanager.leveldb-state-store.compaction-interval-secs,3600), (yarn.resourcemanager.leveldb-state-store.path,${hadoop.tmp.dir}/yarn/system/rmstore), (yarn.resourcemanager.max-completed-applications,10000), (yarn.resourcemanager.nodemanager-connect-retries,10), (yarn.resourcemanager.nodemanager.minimum.version,NONE), (yarn.resourcemanager.nodemanagers.heartbeat-interval-ms,1000), (yarn.resourcemanager.proxy-user-privileges.enabled,false), (yarn.resourcemanager.recovery.enabled,false), (yarn.resourcemanager.resource-tracker.address,${yarn.resourcemanager.hostname}:8031), (yarn.resourcemanager.resource-tracker.client.thread-count,50), (yarn.resourcemanager.scheduler.address,${yarn.resourcemanager.hostname}:8030), (yarn.resourcemanager.scheduler.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler), (yarn.resourcemanager.scheduler.client.thread-count,50), (yarn.resourcemanager.scheduler.monitor.enable,false), (yarn.resourcemanager.scheduler.monitor.policies,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy), (yarn.resourcemanager.state-store.max-completed-applications,${yarn.resourcemanager.max-completed-applications}), (yarn.resourcemanager.store.class,org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore), (yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size,10), (yarn.resourcemanager.system-metrics-publisher.enabled,false), (yarn.resourcemanager.webapp.address,${yarn.resourcemanager.hostname}:8088), (yarn.resourcemanager.webapp.cross-origin.enabled,false), (yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,true), (yarn.resourcemanager.webapp.https.address,${yarn.resourcemanager.hostname}:8090), (yarn.resourcemanager.work-preserving-recovery.enabled,true), (yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms,10000), (yarn.resourcemanager.zk-acl,world:anyone:rwcda), (yarn.resourcemanager.zk-num-retries,1000), (yarn.resourcemanager.zk-retry-interval-ms,1000), (yarn.resourcemanager.zk-state-store.parent-path,/rmstore), (yarn.resourcemanager.zk-timeout-ms,10000), (yarn.scheduler.maximum-allocation-mb,8192), (yarn.scheduler.maximum-allocation-vcores,32), (yarn.scheduler.minimum-allocation-mb,1024), (yarn.scheduler.minimum-allocation-vcores,1), (yarn.sharedcache.admin.address,0.0.0.0:8047), (yarn.sharedcache.admin.thread-count,1), (yarn.sharedcache.app-checker.class,org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker), (yarn.sharedcache.checksum.algo.impl,org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl), (yarn.sharedcache.cleaner.initial-delay-mins,10), (yarn.sharedcache.cleaner.period-mins,1440), (yarn.sharedcache.cleaner.resource-sleep-ms,0), (yarn.sharedcache.client-server.address,0.0.0.0:8045), (yarn.sharedcache.client-server.thread-count,50), (yarn.sharedcache.enabled,false), (yarn.sharedcache.nested-level,3), (yarn.sharedcache.nm.uploader.replication.factor,10), (yarn.sharedcache.nm.uploader.thread-count,20), (yarn.sharedcache.root-dir,/sharedcache), (yarn.sharedcache.store.class,org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore), (yarn.sharedcache.store.in-memory.check-period-mins,720), (yarn.sharedcache.store.in-memory.initial-delay-mins,10), (yarn.sharedcache.store.in-memory.staleness-period-mins,10080), (yarn.sharedcache.uploader.server.address,0.0.0.0:8046), (yarn.sharedcache.uploader.server.thread-count,50), (yarn.sharedcache.webapp.address,0.0.0.0:8788), (yarn.timeline-service.address,${yarn.timeline-service.hostname}:10200), (yarn.timeline-service.client.best-effort,false), (yarn.timeline-service.client.max-retries,30), (yarn.timeline-service.client.retry-interval-ms,1000), (yarn.timeline-service.enabled,false), (yarn.timeline-service.generic-application-history.max-applications,10000), (yarn.timeline-service.handler-thread-count,10), (yarn.timeline-service.hostname,0.0.0.0), (yarn.timeline-service.http-authentication.simple.anonymous.allowed,true), (yarn.timeline-service.http-authentication.type,simple), (yarn.timeline-service.keytab,/etc/krb5.keytab), (yarn.timeline-service.leveldb-state-store.path,${hadoop.tmp.dir}/yarn/timeline), (yarn.timeline-service.leveldb-timeline-store.path,${hadoop.tmp.dir}/yarn/timeline), (yarn.timeline-service.leveldb-timeline-store.read-cache-size,104857600), (yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size,10000), (yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size,10000), (yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms,300000), (yarn.timeline-service.recovery.enabled,false), (yarn.timeline-service.state-store-class,org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore), (yarn.timeline-service.store-class,org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore), (yarn.timeline-service.ttl-enable,true), (yarn.timeline-service.ttl-ms,604800000), (yarn.timeline-service.webapp.address,${yarn.timeline-service.hostname}:8188), (yarn.timeline-service.webapp.https.address,${yarn.timeline-service.hostname}:8190)), System Properties -> Vector((SPARK_SUBMIT,true), (awt.toolkit,sun.awt.X11.XToolkit), (file.encoding,UTF-8), (file.encoding.pkg,sun.io), (file.separator,/), (java.awt.graphicsenv,sun.awt.X11GraphicsEnvironment), (java.awt.printerjob,sun.print.PSPrinterJob), (java.class.version,52.0), (java.endorsed.dirs,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed), (java.ext.dirs,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext), (java.home,/usr/lib/jvm/java-8-openjdk-amd64/jre), (java.io.tmpdir,/tmp), (java.library.path,/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib), (java.runtime.name,OpenJDK Runtime Environment), (java.runtime.version,1.8.0_252-8u252-b09-1ubuntu1-b09), (java.specification.name,Java Platform API Specification), (java.specification.vendor,Oracle Corporation), (java.specification.version,1.8), (java.vendor,Private Build), (java.vendor.url,http://java.oracle.com/), (java.vendor.url.bug,http://bugreport.sun.com/bugreport/), (java.version,1.8.0_252), (java.vm.info,mixed mode), (java.vm.name,OpenJDK 64-Bit Server VM), (java.vm.specification.name,Java Virtual Machine Specification), (java.vm.specification.vendor,Oracle Corporation), (java.vm.specification.version,1.8), (java.vm.vendor,Private Build), (java.vm.version,25.252-b09), (jetty.git.hash,e1bc35120a6617ee3df052294e433f3a25ce7097), (line.separator,
[2020-07-28 16:15:00,011] {logging_mixin.py:112} INFO - [2020-07-28 16:15:00,011] {spark_submit_hook.py:479} INFO - ), (os.arch,amd64), (os.name,Linux), (os.version,5.4.0-40-generic), (path.separator,:), (sun.arch.data.model,64), (sun.boot.class.path,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes), (sun.boot.library.path,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64), (sun.cpu.endian,little), (sun.cpu.isalist,), (sun.desktop,gnome), (sun.io.unicode.encoding,UnicodeLittle), (sun.java.command,org.apache.spark.deploy.SparkSubmit --master local --name airflow-spark /home/bhakti/airflow/spark_jobs/spark_test.py), (sun.java.launcher,SUN_STANDARD), (sun.jnu.encoding,UTF-8), (sun.management.compiler,HotSpot 64-Bit Tiered Compilers), (sun.nio.ch.bugLevel,), (sun.os.patch.level,unknown), (user.country,IN), (user.dir,/home/bhakti/airflow), (user.home,/home/bhakti), (user.language,en), (user.name,bhakti), (user.timezone,Asia/Kolkata)), JVM Information -> List((Java Home,/usr/lib/jvm/java-8-openjdk-amd64/jre), (Java Version,1.8.0_252 (Private Build)), (Scala Version,version 2.12.10)))) by listener AppStatusListener took 2.826632359s.
[2020-07-28 16:15:01,148] {logging_mixin.py:112} INFO - [2020-07-28 16:15:01,148] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:01 INFO AsyncEventQueue: Process of event SparkListenerApplicationStart(Spark_Test_Python,Some(local-1595933069978),1595933035416,bhakti,None,None,None) by listener AppStatusListener took 1.138291811s.
[2020-07-28 16:15:24,854] {logging_mixin.py:112} INFO - [2020-07-28 16:15:24,854] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 241.5 KiB, free 366.1 MiB)
[2020-07-28 16:15:32,169] {logging_mixin.py:112} INFO - [2020-07-28 16:15:32,169] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 366.0 MiB)
[2020-07-28 16:15:32,191] {logging_mixin.py:112} INFO - [2020-07-28 16:15:32,191] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.43.94:38427 (size: 23.4 KiB, free: 366.3 MiB)
[2020-07-28 16:15:32,319] {logging_mixin.py:112} INFO - [2020-07-28 16:15:32,319] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:32 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
[2020-07-28 16:15:33,498] {logging_mixin.py:112} INFO - [2020-07-28 16:15:33,497] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:33 INFO AsyncEventQueue: Process of event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 192.168.43.94, 38427, None),broadcast_0_piece0,StorageLevel(memory, 1 replicas),23956,0)) by listener AppStatusListener took 1.17934701s.
[2020-07-28 16:15:48,231] {logging_mixin.py:112} INFO - [2020-07-28 16:15:48,231] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:48 INFO FileInputFormat: Total input paths to process : 1
[2020-07-28 16:15:51,909] {logging_mixin.py:112} INFO - [2020-07-28 16:15:51,909] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:51 INFO SparkContext: Starting job: collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8
[2020-07-28 16:15:52,701] {logging_mixin.py:112} INFO - [2020-07-28 16:15:52,701] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:52 INFO DAGScheduler: Got job 0 (collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8) with 1 output partitions
[2020-07-28 16:15:52,702] {logging_mixin.py:112} INFO - [2020-07-28 16:15:52,702] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:52 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8)
[2020-07-28 16:15:52,703] {logging_mixin.py:112} INFO - [2020-07-28 16:15:52,703] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:52 INFO DAGScheduler: Parents of final stage: List()
[2020-07-28 16:15:52,705] {logging_mixin.py:112} INFO - [2020-07-28 16:15:52,705] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:52 INFO DAGScheduler: Missing parents: List()
[2020-07-28 16:15:52,727] {logging_mixin.py:112} INFO - [2020-07-28 16:15:52,727] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:52 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8), which has no missing parents
[2020-07-28 16:15:54,400] {logging_mixin.py:112} INFO - [2020-07-28 16:15:54,400] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.4 KiB, free 366.0 MiB)
[2020-07-28 16:15:54,405] {logging_mixin.py:112} INFO - [2020-07-28 16:15:54,405] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 366.0 MiB)
[2020-07-28 16:15:54,406] {logging_mixin.py:112} INFO - [2020-07-28 16:15:54,406] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.43.94:38427 (size: 4.4 KiB, free: 366.3 MiB)
[2020-07-28 16:15:54,407] {logging_mixin.py:112} INFO - [2020-07-28 16:15:54,407] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
[2020-07-28 16:15:54,667] {logging_mixin.py:112} INFO - [2020-07-28 16:15:54,666] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8) (first 15 tasks are for partitions Vector(0))
[2020-07-28 16:15:54,669] {logging_mixin.py:112} INFO - [2020-07-28 16:15:54,669] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
[2020-07-28 16:15:55,640] {logging_mixin.py:112} INFO - [2020-07-28 16:15:55,639] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:55 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1595933152705,WrappedArray(org.apache.spark.scheduler.StageInfo@1d72fa6a),{callSite.short=collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8, spark.rdd.scope={"id":"1","name":"collect"}, spark.rdd.scope.noOverride=true}) by listener AppStatusListener took 2.931189499s.
[2020-07-28 16:15:59,015] {logging_mixin.py:112} INFO - [2020-07-28 16:15:59,014] {spark_submit_hook.py:479} INFO - 20/07/28 16:15:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.43.94, executor driver, partition 0, PROCESS_LOCAL, 7388 bytes)
[2020-07-28 16:16:00,065] {logging_mixin.py:112} INFO - [2020-07-28 16:16:00,065] {spark_submit_hook.py:479} INFO - 20/07/28 16:16:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2020-07-28 16:16:05,983] {logging_mixin.py:112} INFO - [2020-07-28 16:16:05,983] {spark_submit_hook.py:479} INFO - 20/07/28 16:16:05 INFO HadoopRDD: Input split: file:/home/bhakti/airflow/input_data/sample.csv:0+93
[2020-07-28 16:17:06,739] {logging_mixin.py:112} INFO - [2020-07-28 16:17:03,961] {base_job.py:205} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 774, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 546, in do_commit
    dbapi_connection.commit()
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/base_job.py", line 173, in heartbeat
    previous_heartbeat = self.latest_heartbeat
  File "/usr/lib/python3.8/contextlib.py", line 120, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/db.py", line 45, in create_session
    session.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 1042, in commit
    self.transaction.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 508, in commit
    t[1].commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1765, in commit
    self._do_commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1796, in _do_commit
    self.connection._commit_impl()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 776, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 774, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 546, in do_commit
    dbapi_connection.commit()
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked
(Background on this error at: http://sqlalche.me/e/13/e3q8)
[2020-07-28 16:17:07,796] {logging_mixin.py:112} INFO - [2020-07-28 16:17:07,795] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:07 INFO PythonRunner: Times: total = 57347, boot = 56929, init = 418, finish = 0
[2020-07-28 16:17:09,044] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,044] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1822 bytes result sent to driver
[2020-07-28 16:17:09,150] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,150] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 73256 ms on 192.168.43.94 (executor driver) (1/1)
[2020-07-28 16:17:09,170] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,170] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2020-07-28 16:17:09,220] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,219] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 45589
[2020-07-28 16:17:09,270] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,269] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO DAGScheduler: ResultStage 0 (collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8) finished in 75.500 s
[2020-07-28 16:17:09,497] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,497] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2020-07-28 16:17:09,499] {logging_mixin.py:112} INFO - [2020-07-28 16:17:09,498] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2020-07-28 16:17:10,372] {logging_mixin.py:112} INFO - [2020-07-28 16:17:10,372] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:10 INFO DAGScheduler: Job 0 finished: collect at /home/bhakti/airflow/spark_jobs/spark_test.py:8, took 78.461375 s
[2020-07-28 16:17:11,055] {logging_mixin.py:112} INFO - [2020-07-28 16:17:11,055] {spark_submit_hook.py:479} INFO - [('PERSON', 'JOINING_YEAR', 'COMPANY'), ('John', '2018', 'Google'), ('Sam', '2017', 'IBM'), ('Jeff', '2014', 'Yahoo'), ('Allen', '2015', 'Facebook')]
[2020-07-28 16:17:24,255] {logging_mixin.py:112} INFO - [2020-07-28 16:17:21,760] {base_job.py:205} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/base_job.py", line 173, in heartbeat
    previous_heartbeat = self.latest_heartbeat
  File "/usr/lib/python3.8/contextlib.py", line 120, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/db.py", line 45, in create_session
    session.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 1042, in commit
    self.transaction.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 504, in commit
    self._prepare_impl()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 483, in _prepare_impl
    self.session.flush()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 2523, in flush
    self._flush(objects)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 2664, in _flush
    transaction.rollback(_capture_exception=True)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.raise_(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 2624, in _flush
    flush_context.execute()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/unitofwork.py", line 586, in execute
    persistence.save_obj(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py", line 230, in save_obj
    _emit_update_statements(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py", line 994, in _emit_update_statements
    c = cached_connections[connection].execute(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1014, in execute
    return meth(self, multiparams, params)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1127, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked
[SQL: UPDATE job SET latest_heartbeat=? WHERE job.id = ?]
[parameters: ('2020-07-28 10:47:11.741275', 1214)]
(Background on this error at: http://sqlalche.me/e/13/e3q8)
[2020-07-28 16:17:29,268] {logging_mixin.py:112} INFO - [2020-07-28 16:17:29,267] {base_job.py:205} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 774, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 546, in do_commit
    dbapi_connection.commit()
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/base_job.py", line 173, in heartbeat
    previous_heartbeat = self.latest_heartbeat
  File "/usr/lib/python3.8/contextlib.py", line 120, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/db.py", line 45, in create_session
    session.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 1042, in commit
    self.transaction.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 508, in commit
    t[1].commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1765, in commit
    self._do_commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1796, in _do_commit
    self.connection._commit_impl()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 776, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 774, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 546, in do_commit
    dbapi_connection.commit()
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked
(Background on this error at: http://sqlalche.me/e/13/e3q8)
[2020-07-28 16:17:38,642] {logging_mixin.py:112} INFO - [2020-07-28 16:17:38,642] {spark_submit_hook.py:479} INFO - 20/07/28 16:17:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.43.94:38427 in memory (size: 4.4 KiB, free: 366.3 MiB)
[2020-07-28 16:21:07,543] {logging_mixin.py:112} INFO - [2020-07-28 16:21:07,421] {spark_submit_hook.py:479} INFO - 20/07/28 16:21:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/bhakti/airflow/spark-warehouse').
[2020-07-28 16:21:08,196] {logging_mixin.py:112} INFO - [2020-07-28 16:21:08,195] {spark_submit_hook.py:479} INFO - 20/07/28 16:21:07 INFO SharedState: Warehouse path is 'file:/home/bhakti/airflow/spark-warehouse'.
[2020-07-28 16:28:53,841] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,841] {spark_submit_hook.py:479} INFO - 20/07/28 16:28:53 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(0,showString at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
[2020-07-28 16:28:53,903] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,903] {spark_submit_hook.py:479} INFO - sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2020-07-28 16:28:53,903] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,903] {spark_submit_hook.py:479} INFO - sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2020-07-28 16:28:53,903] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,903] {spark_submit_hook.py:479} INFO - sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2020-07-28 16:28:53,903] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,903] {spark_submit_hook.py:479} INFO - java.lang.reflect.Method.invoke(Method.java:498)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - py4j.Gateway.invoke(Gateway.java:282)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - py4j.commands.CallCommand.execute(CallCommand.java:79)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - py4j.GatewayConnection.run(GatewayConnection.java:238)
[2020-07-28 16:28:53,904] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,904] {spark_submit_hook.py:479} INFO - java.lang.Thread.run(Thread.java:748),== Parsed Logical Plan ==
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - GlobalLimit 21
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - +- LocalLimit 21
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - +- Project [cast(_1#0 as string) AS _1#9, cast(_2#1 as string) AS _2#10, cast(_3#2 as string) AS _3#11]
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - +- LogicalRDD [_1#0, _2#1, _3#2], false
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - == Analyzed Logical Plan ==
[2020-07-28 16:28:53,905] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - _1: string, _2: string, _3: string
[2020-07-28 16:28:53,906] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,905] {spark_submit_hook.py:479} INFO - GlobalLimit 21
[2020-07-28 16:28:53,906] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,906] {spark_submit_hook.py:479} INFO - +- LocalLimit 21
[2020-07-28 16:28:53,906] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,906] {spark_submit_hook.py:479} INFO - +- Project [cast(_1#0 as string) AS _1#9, cast(_2#1 as string) AS _2#10, cast(_3#2 as string) AS _3#11]
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,906] {spark_submit_hook.py:479} INFO - +- LogicalRDD [_1#0, _2#1, _3#2], false
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - == Optimized Logical Plan ==
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - GlobalLimit 21
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - +- LocalLimit 21
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - +- LogicalRDD [_1#0, _2#1, _3#2], false
[2020-07-28 16:28:53,907] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:28:53,908] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,907] {spark_submit_hook.py:479} INFO - == Physical Plan ==
[2020-07-28 16:28:53,908] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,908] {spark_submit_hook.py:479} INFO - CollectLimit 21
[2020-07-28 16:28:53,908] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,908] {spark_submit_hook.py:479} INFO - +- *(1) Scan ExistingRDD[_1#0,_2#1,_3#2]
[2020-07-28 16:28:53,908] {logging_mixin.py:112} INFO - [2020-07-28 16:28:53,908] {spark_submit_hook.py:479} INFO - ,org.apache.spark.sql.execution.SparkPlanInfo@b807ee,1595933927445) by listener SQLAppStatusListener took 6.318343091s.
[2020-07-28 16:29:09,518] {logging_mixin.py:112} INFO - [2020-07-28 16:29:09,518] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:09 INFO CodeGenerator: Code generated in 11803.081446 ms
[2020-07-28 16:29:10,089] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,088] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2020-07-28 16:29:10,092] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,091] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2020-07-28 16:29:10,093] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,093] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2020-07-28 16:29:10,093] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,093] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO DAGScheduler: Parents of final stage: List()
[2020-07-28 16:29:10,094] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,094] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO DAGScheduler: Missing parents: List()
[2020-07-28 16:29:10,096] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,096] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2020-07-28 16:29:10,116] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,116] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.2 KiB, free 366.0 MiB)
[2020-07-28 16:29:10,758] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,758] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)
[2020-07-28 16:29:10,760] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,759] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.43.94:38427 (size: 5.8 KiB, free: 366.3 MiB)
[2020-07-28 16:29:10,761] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,760] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
[2020-07-28 16:29:10,762] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,762] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2020-07-28 16:29:10,763] {logging_mixin.py:112} INFO - [2020-07-28 16:29:10,763] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
[2020-07-28 16:29:11,213] {logging_mixin.py:112} INFO - [2020-07-28 16:29:11,213] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.43.94, executor driver, partition 0, PROCESS_LOCAL, 7493 bytes)
[2020-07-28 16:29:11,215] {logging_mixin.py:112} INFO - [2020-07-28 16:29:11,215] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2020-07-28 16:29:13,120] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,120] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO PythonRunner: Times: total = 16, boot = 6, init = 9, finish = 1
[2020-07-28 16:29:13,128] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,128] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2235 bytes result sent to driver
[2020-07-28 16:29:13,131] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,131] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2366 ms on 192.168.43.94 (executor driver) (1/1)
[2020-07-28 16:29:13,132] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,132] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2020-07-28 16:29:13,134] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,134] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 3.035 s
[2020-07-28 16:29:13,135] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,134] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2020-07-28 16:29:13,135] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,135] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2020-07-28 16:29:13,136] {logging_mixin.py:112} INFO - [2020-07-28 16:29:13,136] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:13 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 3.046808 s
[2020-07-28 16:29:14,260] {logging_mixin.py:112} INFO - [2020-07-28 16:29:14,259] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:14 INFO CodeGenerator: Code generated in 302.185151 ms
[2020-07-28 16:29:17,139] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,139] {spark_submit_hook.py:479} INFO - +------+------------+--------+
[2020-07-28 16:29:17,140] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,139] {spark_submit_hook.py:479} INFO - |    _1|          _2|      _3|
[2020-07-28 16:29:17,140] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,140] {spark_submit_hook.py:479} INFO - +------+------------+--------+
[2020-07-28 16:29:17,140] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,140] {spark_submit_hook.py:479} INFO - |PERSON|JOINING_YEAR| COMPANY|
[2020-07-28 16:29:17,141] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,141] {spark_submit_hook.py:479} INFO - |  John|        2018|  Google|
[2020-07-28 16:29:17,141] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,141] {spark_submit_hook.py:479} INFO - |   Sam|        2017|     IBM|
[2020-07-28 16:29:17,141] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,141] {spark_submit_hook.py:479} INFO - |  Jeff|        2014|   Yahoo|
[2020-07-28 16:29:17,142] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,142] {spark_submit_hook.py:479} INFO - | Allen|        2015|Facebook|
[2020-07-28 16:29:17,142] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,142] {spark_submit_hook.py:479} INFO - +------+------------+--------+
[2020-07-28 16:29:17,142] {logging_mixin.py:112} INFO - [2020-07-28 16:29:17,142] {spark_submit_hook.py:479} INFO - 
[2020-07-28 16:29:25,970] {logging_mixin.py:112} INFO - [2020-07-28 16:29:25,970] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.43.94:38427 in memory (size: 5.8 KiB, free: 366.3 MiB)
[2020-07-28 16:29:26,058] {logging_mixin.py:112} INFO - [2020-07-28 16:29:26,058] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:26 INFO SparkContext: Invoking stop() from shutdown hook
[2020-07-28 16:29:26,393] {logging_mixin.py:112} INFO - [2020-07-28 16:29:26,393] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:26 INFO SparkUI: Stopped Spark web UI at http://192.168.43.94:4042
[2020-07-28 16:29:26,891] {logging_mixin.py:112} INFO - [2020-07-28 16:29:26,891] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2020-07-28 16:29:28,790] {logging_mixin.py:112} INFO - [2020-07-28 16:29:28,789] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:28 INFO MemoryStore: MemoryStore cleared
[2020-07-28 16:29:28,794] {logging_mixin.py:112} INFO - [2020-07-28 16:29:28,794] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:28 INFO BlockManager: BlockManager stopped
[2020-07-28 16:29:29,143] {logging_mixin.py:112} INFO - [2020-07-28 16:29:29,143] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2020-07-28 16:29:30,057] {logging_mixin.py:112} INFO - [2020-07-28 16:29:30,056] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2020-07-28 16:29:31,583] {logging_mixin.py:112} INFO - [2020-07-28 16:29:31,582] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:31 INFO SparkContext: Successfully stopped SparkContext
[2020-07-28 16:29:31,583] {logging_mixin.py:112} INFO - [2020-07-28 16:29:31,583] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:31 INFO ShutdownHookManager: Shutdown hook called
[2020-07-28 16:29:31,586] {logging_mixin.py:112} INFO - [2020-07-28 16:29:31,585] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-830a073c-e0fd-42c3-a7be-4d5f6eb3afdf/pyspark-fd022f70-04d2-4720-a053-794f790a1321
[2020-07-28 16:29:31,590] {logging_mixin.py:112} INFO - [2020-07-28 16:29:31,590] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-830a073c-e0fd-42c3-a7be-4d5f6eb3afdf
[2020-07-28 16:29:31,593] {logging_mixin.py:112} INFO - [2020-07-28 16:29:31,593] {spark_submit_hook.py:479} INFO - 20/07/28 16:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7ec5c58-cda5-481b-a0ca-aaafbdda8074
[2020-07-28 16:29:31,660] {taskinstance.py:1052} INFO - Marking task as SUCCESS.dag_id=submit_data, task_id=spark_submit, execution_date=20200721T022900, start_date=20200728T104329, end_date=20200728T105931
[2020-07-28 16:29:42,013] {logging_mixin.py:112} INFO - [2020-07-28 16:29:39,945] {base_job.py:205} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/base_job.py", line 173, in heartbeat
    previous_heartbeat = self.latest_heartbeat
  File "/usr/lib/python3.8/contextlib.py", line 120, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/db.py", line 45, in create_session
    session.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 1042, in commit
    self.transaction.commit()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 504, in commit
    self._prepare_impl()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 483, in _prepare_impl
    self.session.flush()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 2523, in flush
    self._flush(objects)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 2664, in _flush
    transaction.rollback(_capture_exception=True)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.raise_(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py", line 2624, in _flush
    flush_context.execute()
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/unitofwork.py", line 586, in execute
    persistence.save_obj(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py", line 230, in save_obj
    _emit_update_statements(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py", line 994, in _emit_update_statements
    c = cached_connections[connection].execute(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1014, in execute
    return meth(self, multiparams, params)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1127, in _execute_clauseelement
    ret = self._execute_context(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    self._handle_dbapi_exception(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    util.raise_(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked
[SQL: UPDATE job SET latest_heartbeat=? WHERE job.id = ?]
[parameters: ('2020-07-28 10:59:29.922159', 1214)]
(Background on this error at: http://sqlalche.me/e/13/e3q8)
[2020-07-28 16:29:42,014] {logging_mixin.py:112} INFO - [2020-07-28 16:29:42,014] {local_task_job.py:103} INFO - Task exited with return code 0
